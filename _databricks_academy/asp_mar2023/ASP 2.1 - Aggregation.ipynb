{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a22df98-b15b-46ab-9858-9c8c40cb053c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee01bc2-6c7d-4a86-a47a-98ee9331cfcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregation\n",
    "\n",
    "##### Objectives\n",
    "1. Group data by specified columns\n",
    "1. Apply grouped data methods to aggregate data\n",
    "1. Apply built-in functions to aggregate data\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `groupBy`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.html#pyspark.sql.GroupedData\" target=\"_blank\" target=\"_blank\">Grouped Data</a>: `agg`, `avg`, `count`, `max`, `sum`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>: `approx_count_distinct`, `avg`, `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e98f2c-9e0c-4656-8e03-4a1b3cdbe72d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28ac5676-fafc-42be-9107-cb1bb89491b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use the BedBricks events dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36df7c24-62b6-400c-ac7a-96a307fe8352",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(eventsPath)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "384a75a3-059c-42ea-85b6-adac8863956a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Grouping data\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/aggregation_groupby.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5976fc-7e7a-4e3f-b955-383aff2dd1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### groupBy\n",
    "Use the DataFrame `groupBy` method to create a grouped data object. \n",
    "\n",
    "This grouped data object is called `RelationalGroupedDataset` in Scala and `GroupedData` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0ba168-bb34-4f03-9c4b-914420013e60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"event_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b276409-fee4-4c3c-8daa-94122469c4de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"geo.state\", \"geo.city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f2cec0-b2da-4778-ac84-f522956c2c43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Grouped data methods\n",
    "Various aggregation methods are available on the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.html\" target=\"_blank\">GroupedData</a> object.\n",
    "\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| agg | Compute aggregates by specifying a series of aggregate columns |\n",
    "| avg | Compute the mean value for each numeric columns for each group |\n",
    "| count | Count the number of rows for each group |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| min | Compute the min value for each numeric column for each group |\n",
    "| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n",
    "| sum | Compute the sum for each numeric columns for each group |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a4fa7f-c72d-438c-a792-789bfe490378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventCountsDF = df.groupBy(\"event_name\").count()\n",
    "display(eventCountsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fab95e5-6d47-4ca2-968b-290edc95bbd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, we're getting the average purchase revenue for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1a81a5-e79c-4ce9-9435-eb1ca7b97788",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avgStatePurchasesDF = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\n",
    "display(avgStatePurchasesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6582e3e0-b040-4ed2-ad74-f802f23b39d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And here the total quantity and sum of the purchase revenue for each combination of state and city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63f1e907-05f6-40b9-97cf-1328d68452ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cityPurchaseQuantitiesDF = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\n",
    "display(cityPurchaseQuantitiesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b0386c-46a2-4c63-b7c3-46faa6fc19c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Built-In Functions\n",
    "In addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-functions-builtin.html\" target=\"_blank\">SQL functions</a> module.\n",
    "\n",
    "In Scala, this is <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_bank\">`org.apache.spark.sql.functions`</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\" target=\"_blank\">`pyspark.sql.functions`</a> in Python. Functions from this module must be imported into your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586d1851-f3ea-4cac-814e-1eb9fe77601e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aggregate Functions\n",
    "\n",
    "Here are some of the built-in functions available for aggregation.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| approx_count_distinct | Returns the approximate number of distinct items in a group |\n",
    "| avg | Returns the average of the values in a group |\n",
    "| collect_list | Returns a list of objects with duplicates |\n",
    "| corr | Returns the Pearson Correlation Coefficient for two columns |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| stddev_samp | Returns the sample standard deviation of the expression in a group |\n",
    "| sumDistinct | Returns the sum of distinct values in the expression |\n",
    "| var_pop | Returns the population variance of the values in a group |\n",
    "\n",
    "Use the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">`agg`</a> to apply built-in aggregate functions\n",
    "\n",
    "This allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">`alias`</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd15e219-bf86-4c70-aef6-48c0bbb2c0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "statePurchasesDF = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\n",
    "display(statePurchasesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6a5dc2-0d65-4ff6-a9ad-30b29d998e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apply multiple aggregate functions on grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d44a053-63df-4546-a883-d824cc16a33b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct\n",
    "\n",
    "stateAggregatesDF = (df\n",
    "                     .groupBy(\"geo.state\")\n",
    "                     .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n",
    "                          approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n",
    "                    )\n",
    "\n",
    "display(stateAggregatesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f99902-6d23-400d-b371-87f3aebc84b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Math Functions\n",
    "Here are some of the built-in functions for math operations.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| ceil | Computes the ceiling of the given column. |\n",
    "| cos | Computes the cosine of the given value. |\n",
    "| log | Computes the natural logarithm of the given value. |\n",
    "| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n",
    "| sqrt | Computes the square root of the specified float value. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e2e69a1-5c5e-4b95-ba6e-fa6ec399f137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import cos, sqrt\n",
    "\n",
    "display(\n",
    "    spark.range(10)  # Create a DataFrame with a single column called \"id\" with a range of integer values\n",
    "    .withColumn(\"sqrt\", sqrt(\"id\"))\n",
    "    .withColumn(\"cos\", cos(\"id\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ef7fb3e-a13a-439b-b62f-f9b5ddb3b6e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Revenue by Traffic Lab\n",
    "Get the 3 traffic sources generating the highest total revenue.\n",
    "1. Aggregate revenue by traffic source\n",
    "2. Get top 3 traffic sources by total revenue\n",
    "3. Clean revenue columns to have two decimal places\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: groupBy, sort, limit\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html?highlight=column#pyspark.sql.Column\" target=\"_blank\">Column</a>: alias, desc, cast, operators\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-in Functions</a>: avg, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d613f16-ce8f-42f4-9d07-596a97c26182",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "Run the cell below to create the starting DataFrame **`df`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7505ce2a-27c6-4f46-896d-2f8bbe7d242a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Purchase events logged on the BedBricks website\n",
    "df = (spark.read.parquet(eventsPath)\n",
    "      .withColumn(\"revenue\", col(\"ecommerce.purchase_revenue_in_usd\"))\n",
    "      .filter(col(\"revenue\").isNotNull())\n",
    "      .drop(\"event_name\")\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba58a156-66b5-44dd-9d39-bdb5753f90da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Aggregate revenue by traffic source\n",
    "- Group by **`traffic_source`**\n",
    "- Get sum of **`revenue`** as **`total_rev`**\n",
    "- Get average of **`revenue`** as **`avg_rev`**\n",
    "\n",
    "Remember to import any necessary built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc33f757-46f1-459e-806a-ff703faeec6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "trafficDF = (df.FILL_IN\n",
    ")\n",
    "\n",
    "display(trafficDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e00b1586-0c88-418b-8763-97bc042a6b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35ecd875-aea6-4862-9aec-af3d0a5b48a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "expected1 = [(12704560.0, 1083.175), (78800000.3, 983.2915), (24797837.0, 1076.6221), (47218429.0, 1086.8303), (16177893.0, 1083.4378), (8044326.0, 1087.218)]\n",
    "testDF = trafficDF.sort(\"traffic_source\").select(round(\"total_rev\", 4).alias(\"total_rev\"), round(\"avg_rev\", 4).alias(\"avg_rev\"))\n",
    "result1 = [(row.total_rev, row.avg_rev) for row in testDF.collect()]\n",
    "\n",
    "assert(expected1 == result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097e05d7-da9f-4a29-bbb3-d36e249f6a9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Get top three traffic sources by total revenue\n",
    "- Sort by **`total_rev`** in descending order\n",
    "- Limit to first three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76e2acba-d76f-44f9-a8bb-45d5e6d047fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "topTrafficDF = (trafficDF.FILL_IN\n",
    ")\n",
    "display(topTrafficDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6015178-13f7-441e-8fa5-c0681b7429f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18cc7a5-9b1a-4602-8353-666700e40869",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected2 = [(78800000.3, 983.2915), (47218429.0, 1086.8303), (24797837.0, 1076.6221)]\n",
    "testDF = topTrafficDF.select(round(\"total_rev\", 4).alias(\"total_rev\"), round(\"avg_rev\", 4).alias(\"avg_rev\"))\n",
    "result2 = [(row.total_rev, row.avg_rev) for row in testDF.collect()]\n",
    "\n",
    "assert(expected2 == result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcef21d-5948-4c68-95db-2e540e4acb62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Limit revenue columns to two decimal places\n",
    "- Modify columns **`avg_rev`** and **`total_rev`** to contain numbers with two decimal places\n",
    "  - Use **`withColumn()`** with the same names to replace these columns\n",
    "  - To limit to two decimal places, multiply each column by 100, cast to long, and then divide by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae0f4185-3f0c-4ced-9279-1880f888331a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "finalDF = (topTrafficDF.FILL_IN\n",
    ")\n",
    "\n",
    "display(finalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65bb7058-8c65-4cca-873c-4958eaef33f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6614a97d-3305-4df9-9d88-1ad041ae0af0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected3 = [(78800000.29, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\n",
    "result3 = [(row.total_rev, row.avg_rev) for row in finalDF.collect()]\n",
    "\n",
    "assert(expected3 == result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba48c3b7-202f-4571-8966-813dbb74d9af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Bonus: Rewrite using a built-in math function\n",
    "Find a built-in math function that rounds to a specified number of decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67372100-2f96-4c19-8b39-31567e9b022d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "bonusDF = (topTrafficDF.FILL_IN\n",
    ")\n",
    "\n",
    "display(bonusDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "644e9146-ceab-42e8-9dd1-5470d090263b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02551e49-3a68-4f6a-a4b0-06078b6f6701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected4 = [(78800000.3, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\n",
    "result4 = [(row.total_rev, row.avg_rev) for row in bonusDF.collect()]\n",
    "\n",
    "assert(expected4 == result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6521b17c-e57b-4262-82f0-8891565ce8d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Chain all the steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3420d886-2aad-4735-9db2-69939130753b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "chainDF = (df.FILL_IN\n",
    ")\n",
    "\n",
    "display(chainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "913f11ff-a69e-4af9-984f-49e3ec8c94f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd2186e1-550f-4882-a234-88fb0ea30cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected5 = [(78800000.3, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\n",
    "result5 = [(row.total_rev, row.avg_rev) for row in chainDF.collect()]\n",
    "\n",
    "assert(expected5 == result5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a85e2d9-7852-446b-9768-433965c92428",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean up classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6be3db5-7a41-4108-a91e-5e26b4d664f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56d10559-0ba0-473b-90ad-f1396f8642cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ASP 2.1 - Aggregation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
