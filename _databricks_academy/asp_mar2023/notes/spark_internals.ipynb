{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Spark Cluster, Spark Execution\n",
    "\n",
    "Spark cluster parable of classroom of teacher & students\n",
    "- each table of students is like an executor & each student is a \"core\"\n",
    "    - Each \"core\" is given an individual task.\n",
    "\n",
    "Scenario 2: count total pieces in \"candy bags\"\n",
    "- stage1: local count, each partition gets distributed to a core on a different executor.\n",
    "    - driver takes result of which ever executor finishes first, then the rest of the executors commit their results after\n",
    "    - Then stage1 is complete\n",
    "- stage2: global count, another executor fetches all the counts from each executor after all the counts complete. These results get passed to the driver, stage2 is then complete.\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Shuffling & Caching\n",
    "\n",
    "groupBy triggers a _Wide operation_. Other wide transformations include:\n",
    "- `distinct, sort, join`\n",
    "\n",
    "_Narrow transformations_ include:\n",
    "- `select, filter, cast, union`\n",
    "\n",
    "Narrow transformations = when the data is required to compute the recs in a single partition that all reside in at most 1 partition of the parent rdd.\n",
    "\n",
    "Wide transformations = when the data is required to compute the records in a single partition that may reside in many partitions of the parent rdd.\n",
    "\n",
    "A shuffle introduces(i.e. demarcates) stage boundaries, shuffles happen when a wide transformation happens.\n",
    "Each shuffle requires a shuffle read (from disk), and a shuffle write (to disk).\n",
    "1st, shuffle writes write to disk so that all subsequent shuffle reads can read those files. Shuffle writes only happen once.\n",
    "\n",
    "\n",
    "### Transcript\n",
    "\n",
    "2m33s: It might help to describe the 1st half of the shuffle just like any other write,\n",
    "just saving the file, then the 2nd half is just another read, like spark.read\n",
    "in this way stage1 and stage2 functions much in the same way; we read the data then write the data.\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Query Optimization\n",
    "\n",
    "### Transcript\n",
    "\n",
    "**0sec:** This lesson introduces query optimization concepts in SparkSQL.\n",
    "You will start by describing the stages of SparksSQLs optimization process\n",
    "when executing a SQL query.\n",
    "Then you will explore the logical and physical plans of a dataframe.\n",
    "Lastly, you will demonstrate logical optimizations and predicate pushdown\n",
    "performed by Spark.\n",
    "By the end of this lesson you should be able to\n",
    "- describe the stages of SparkSQLs optimization process when executing a SQL query,\n",
    "- recognize the logical and physical plans of a dataframe,\n",
    "- and demonstrate logical optimizations and predicate pushdown performed by Spark.\n",
    "Let's get started.\n",
    "\n",
    "**44secs:** fundamental to the SQL and DataFrames API is the Catalyst Optimizer.\n",
    "It's an extensible query optimizer. It contains a general library for\n",
    "representing tree's and applying rules to manipulate them.\n",
    "And it has several public extension points including external data sources\n",
    "and user defined types.\n",
    "\n",
    "**1m9s:** The goal here is to breakdown the diagram as follows,\n",
    "SQL, DataFrame, and Datasets, this is a declarative API.\n",
    "It doesn't matter which API or language you use,\n",
    "in all cases the instructions all boil down to a logical plan\n",
    "\n",
    "**1m34s:** The unresolved logical plan, this is what the developer logically wants to happen.\n",
    "column names, tables names, UDFs, etc. are not resolved.\n",
    "In other words, they may not exist or we may have typos in our code.\n",
    "Then analysis happens, this is where we validate column names, table names, UDFs, etc.\n",
    "against the metadata catalog.\n",
    "From this we get the logical plan, then we have a sanity check,\n",
    "for example we make sure it doesn't refer to non existent columns. Or we have an order by to sort.\n",
    "This is where the first set of logical optimizations take place.\n",
    "We potentially rewrite, reorder, and so on the logical sequence of calls.\n",
    "From this we get the optimized logical plan\n",
    "\n",
    "**2m21s:** Next comes the physical planning, this is where the catalyst optimizer determines there are\n",
    "a multiple of ways to execute a query.\n",
    "For example, do we pull 100% of the data across the network?\n",
    "or do we use a predicate pushdown and filter the data at its' source?\n",
    "Maybe a parquet file or JDBC where clause. Thus maybe only bringing 30% of the data.\n",
    "From this we get 1 or more physical plans.\n",
    "The physical plans represent what the query engine will actually do.\n",
    "These are distinctly different from the logical plan in that,\n",
    "all the optimizations have been applied.\n",
    "Each optimization provides a measurably different benefit. Its' cost model.\n",
    "\n",
    "**3m12s:** In the cost model, we see that each physical plan is evaluated according to the cost model.\n",
    "The best performing model is selected. This gives us our selected physical plan.\n",
    "Finally, we have code generation.\n",
    "\n",
    "**3m29s:** Once all the planning is done, The selected physical plan is compiled down to RDDs.\n",
    "This is the same RDD a developer would write by hand, but it's highly inconceivable a developer\n",
    "could do a better job.\n",
    "Finally, we have execution. Once the RDDs are generated they are executed in the Spark core.\n",
    "\n",
    "**3m53s:** Adaptive Query Execution.\n",
    "AQE creates runtime statistics. These are based on the statistics of the finished plan nodes.\n",
    "And they reoptimize the execution plan of the remaining queries.\n",
    "AQE can do things like dynamically switch join strategies, dynamically coalesce shuffle partitions,\n",
    "or dynamically optimize skew joins.\n",
    "\n",
    "\n",
    "### highlights\n",
    "\n",
    "We can turn on the Adaptive Query Execution to improve the logical plan and physical plan.\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Demo notes\n",
    "\n",
    "Shown an example of how a cache could accidentally block a predicate pushdown.\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transcript\n",
    "\n",
    "   **~~ 0sec, What we will Cover ~~**\n",
    "Let's discuss partitioning. In this lesson you will understand\n",
    "- the relationship between partitions and slots & cores.\n",
    "- You'll configure default shuffle partitions,\n",
    "- describe repartition & coalesce,\n",
    "- match the number of partitions to the number of slots & cores,\n",
    "- and you'll describe dynamic coalescing of shuffle partitions in AQE.\n",
    "\n",
    "____\n",
    "   **~~ 28sec, Cores and Slots ~~**\n",
    "The spark api uses the term \"core\", meaning a thread available for parallel execution.\n",
    "\n",
    "Here we refer to it as a slot to avoid confusion with the number of cores in the underlying CPU.\n",
    "To which there isn't necessarily an equal number.\n",
    "\n",
    "In most cases, if you created a cluster, you should know how many cores you have.\n",
    "However, to check programmatically, you can use:\n",
    "**`spark.sparkContext.defaultParallelism`**\n",
    "\n",
    "____\n",
    "   **~~ 1m2s, Cores in Cluster ~~**\n",
    "For operations like parallelize with no parent RDDs, it depends on the cluster manager.\n",
    "In local mode, you'll have a number of cores on the local machine.\n",
    "Mesos fine grain mode 8 and others have a total number of cores on all executor nodes or 2,\n",
    "whichever is larger.\n",
    "\n",
    "\n",
    "   **~~ 1m21s, Partitions of Data ~~**\n",
    "**`df.rdd.getNumPartitions()`**\n",
    "The 2nd half of this question is how many partitions of data do I have?\n",
    "With that we have 2 subsequent questions:\n",
    "1. Why do I have that many?\n",
    "2. And what is a partition?\n",
    "\n",
    "A partition is a small piece of the total dataset. The action or state of dividing or being divided into parts.\n",
    "If our goal is to process all our data, say 1million records in parallel, we need to divide that data up.\n",
    "If I have 8 slots for parallel execution, it would stand to reason that I want:\n",
    " 1,000,000/8 ...or 125,000 records per partition.\n",
    "\n",
    "Back on that 1st question, we can answer it by running the following command:\n",
    " **`df.rdd.getNumPartitions()`**\n",
    "  which takes the initial dataframe, converts it to an RDD, then asks the RDD for the number of partitions.\n",
    "\n",
    "It is not coincidental that we have 8 slots and 8 partitions.\n",
    "In Spark 2.0 a lot of optimizations had been added to the readers.\n",
    "Namely the readers looked at the number of slots, the size of the data,\n",
    "and made a best guess at the number of partitions that should be created.\n",
    "\n",
    "You could actually double the size of the data several times over in Spark\n",
    "and it would still read in only 8 partitions.\n",
    "Eventually it will get so big that Spark will forego optimization and read it in as 10 partitions (in that case).\n",
    "\n",
    "8 partitions and 8 slots is just too easy, let's read in another copy of the same data.\n",
    "A parquet file that was saved in 5 partitions.\n",
    "This gives us an excuse to reason about the relationship between slots and partitions.\n",
    "\n",
    "   **~~ 3m08s, Repartition a DataFrame ~~**\n",
    "The key difference between the 2 are\n",
    "- Coalesce N is a narrow transformation and can only be used to reduce the number of partitions.\n",
    "- Repartition N is a wide transformation and can be used to increase or decrease the number of partitions.\n",
    "\n",
    "\n",
    "```\n",
    "... from 3m28s-4m14s: I stopped transcribing\n",
    "because everything he says is easy to hear & understand\n",
    "```\n",
    "\n",
    "\n",
    "4m15s: In our case we need to go from 5 partitions up to 8 partitions. Our only option here is repartition.\n",
    "coalesce is faster, but can result in uneven partition sizes.\n",
    "repartition is slower because it requires a shuffle, but results in more evenly distributed partition sizes.\n",
    "\n",
    "   **~~ 4m50s, Make the # of partitions a multiple of the # of cores ~~**\n",
    "Let's make sure we're using every slot and core.\n",
    "With very few exceptions you always want the number of partitions to be a multiple of the number of slots.\n",
    "    e.g. with 4 slots you want 4, 8, 12, 16, 20, 24, 28, 32, etc partitions.\n",
    "That way every slot is used. That is, every slot is being assigned a task.\n",
    "With 5 partitions and 8 slots we are underutilizing 3 of the 8 slots.\n",
    "With 9 partitions and 8 slots we just guaranteed that our job will take 2 times as long as it may need to.\n",
    "For example: 10secs to process the first 8 partitions,\n",
    "    then as soon as the first 8 is done, another 10 seconds to process that last partition.\n",
    "\n",
    "5m25s: you might be asking should I use more or less partitions?\n",
    "As a general guideline, it is advised that each partition when cached, is roughly around 200MB(on disc).\n",
    "Size on the disc though is not a good gauge e.g. CSVs on large on disk, but small in RAM.\n",
    "Consider string \"12345\" which is 10 bytes on disk, but the int 12345 is 4 bytes in RAM.\n",
    "Parquet files are highly compressed, so they are small on disk, but when uncompressed they are large in RAM.\n",
    "\n",
    "6m5s: on an executor with a reduced amount of RAM you might need to lower that.\n",
    "For example at 8 partitions, corresponding to our max number of slots, and 200MB per partition,\n",
    "that will use roughly 1.5GB...\n",
    "If you have transformations that will balloon the data size,\n",
    "such as NLP, you are sure to run into problems.\n",
    "\n",
    "6m35s: a question might arise, if I read my data and it comes in at 10 partitions.\n",
    "Should I reduce my partitions down to 8? Or increase the number of partitions to 16?\n",
    "\n",
    "6m54: The answer is, it depends on the size of each partition.\n",
    "- we'll read the data in, cache it, look at the size of every partition,\n",
    "- if you're over 200MB, consider increasing the number of partitions to 16(to make each partition roughly 100MB).\n",
    "- If you're under 200MB, consider decreasing the number of partitions (to increase the size of each partition).\n",
    "- The goal will always be to use as few partitions as possible,\n",
    "   while maintaining at least 1 times the number of slots.\n",
    "\n",
    "____\n",
    "   **~~ 7m21s, Default Shuffle Partitions ~~**\n",
    "Wide operations have to shuffle the data, once the data is shuffled it has to be re-partitioned.\n",
    "Unlike repartition and coalesce, we did not specify how many partitions to use.\n",
    "The problem is the number of partitions we ended up with.\n",
    "Besides looking at the number of tasks in the final stage, we can simply print out the number of partitions.\n",
    "\n",
    "The default partition size(after a shuffle) is 200MB.\n",
    "This is based on real world experience of Apache Spark Engineers.\n",
    "\n",
    "\n",
    "8m8s: for now we can tweak it with a configuration value:\n",
    "`spark.conf.get(\"spark.sql.shuffle.partitions\")`\n",
    "`spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")`\n",
    "We can change the config setting with this command.\n",
    "\n",
    "\n",
    "   **~~ 8m26s, Partitioning Guidelines ❤️ ~~**\n",
    "Always err on the side of too many small partitions, than too few large partitions.\n",
    "With this rule in mind,\n",
    "- target never letting partition size increase above 200MB per 8GB of slot total memory. A 1 to 40 ratio.\n",
    "- For small data, target 3 partitions per core/slot.\n",
    "- Read and Shuffle tasks should complete in less than 10 seconds on average.\n",
    "- And the target, a medium partition size of approximately 200MB as a starting point.\n",
    "\n",
    "9m1s: Lastly, realize that there is almost always skew in real world datasets,\n",
    "which means even though you target a 200MB partition it is likely several\n",
    "of your partitions will be 2 or more times larger than the 200MB target.\n",
    "And we want to make sure these tasks finish in less than 10 seconds as well whenever possible.\n",
    "\n",
    "9m25s: Shuffles are often the most expensive operation in a Spark job\n",
    "and as such write/right sizing the shuffle partitions are the most crucial.\n",
    "The default shuffle partitions are set to 200,\n",
    "meaning that any shuffled dataset greater than 40GB will violate our maximum shuffle partition size of 200MB.\n",
    "Sizing shuffle partitions is all about knowing 2 key variables:\n",
    "1. the amount of data coming into the larger shuffle stage of an action\n",
    "    so for example, across all the jobs in the action\n",
    "2. And the target size of each shuffle partition.\n",
    "\n",
    "a good target size will still follow our 1 to 40 ratio of partition size, executor slot memory\n",
    "for this example we will use 200MB assuming an 8GB total slot memory\n",
    "`P` is our partition target size of 200MB\n",
    "`I` is our largest shuffle stage input and that is going to be 4TB\n",
    "The equation here that we come up with is `I / P`, our shuffle partition count.\n",
    "e.g.\n",
    "    4TB / 200MB = 20,000 shuffle partition count\n",
    "so we'd set\n",
    "    `spark.conf.set(\"spark.sql.shuffle.partitions\", \"20000\")`\n",
    "which stands for our shuffle partition count.\n",
    "\n",
    "   **~~ 10m44s, 3.0 Adaptive Query Execution ~~**\n",
    "Dynamically coalescing shuffle partitions.\n",
    "when running queries in Spark to deal with very large data,\n",
    "shuffle usually has a very important impact on query performance among other things.\n",
    "Shuffle is an expensive operator as it needs to move data across the network\n",
    "so that data is re-distributed in a way required by downstream operators.\n",
    "One key property of shuffle is the number of partitions.\n",
    "The best number of partitions is data dependent, yet data sizes may differ vastly from stage to stage, query to query.\n",
    "Making this number hard to tune.\n",
    "\n",
    "If there are too few partitions then the data size of each partition may be very large,\n",
    "and the tasks to process these large partitions may need to spill data to disk.\n",
    "e.g. when we sort or there is an aggregate involved.\n",
    "And as a result they will slow down the query.\n",
    "\n",
    "11m41s: If there are too many partitions, then the data size of each partition may be very small,\n",
    "and there will be a lot of small network data fetches to read the shuffle blocks.\n",
    "which can also slowdown the query because of the inefficient I/O pattern.\n",
    "\n",
    "Having a large number of tasks also puts more burden on the spark task scheduler.\n",
    "To solve this problem we can set a relatively large number of partitions,\n",
    "at the beginning. Then combine adjacent small partitions into bigger partitions\n",
    "at runtime by looking at shuffle file statistics.\n",
    "\n",
    "12m12s: So for example, let's say we're running the query \"select max(i) from table group by j\"\n",
    "the input data table is rather small so there are only 2 partitions before grouping.\n",
    "The initial shuffle partition number is set 5 so after local grouping\n",
    "the \"partition-ally\" grouped data is shuffled into 5 partitions.\n",
    "Without AQE, spark will start 5 tasks to do the final aggregation.\n",
    "However, there are 3 very small partitions here and it would be a waste to start a separate task for each of them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "//"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Highlights\n",
    "\n",
    "Understand the relationship between partitions and slots & cores\n",
    "configure default shuffle partitions\n",
    "describe repartition & coalesce\n",
    "match number of partitions to slots & cores\n",
    "describe dynamic coalescing of shuffle partitions in AQE\n",
    "\n",
    "In Spark, a \"core\" is a thread available for parallel execution.\n",
    "We refer to them interchangably as slots to avoid confusion w/cores from the CPU.\n",
    "`spark.sparkContext.defaultParallelism`\n",
    "\n",
    "To get the number of partitions\n",
    "`df.rdd.getNumPartitions()`\n",
    "\n",
    "Let's say we have a pq file that was saved in 5 partitions with 8 available slots\n",
    "We have 2 options to *repartition*: .coalesce(N) or .repartition(N)\n",
    "pro's & con's: shuffle vs non-shuffle, even re-distribution, decrease-only vs increase-decrease\n",
    "\n",
    "We generally want the number of partitions to a multiple of the number of available slots\n",
    "e.g. if we have 4 slots ideally we should have 8 or 12 or 16... partitions\n",
    "\n",
    "A **very general guideline** is to have each partition be roughly about 200MB... ballpark\n",
    "\n",
    "On an executor with a reduced amount of RAM, we might need to lower the 200MB estimate. \n",
    "e.g. at 8 partitions corresponding to 4 slots, we would use close to 1.5GB \n",
    "If there are a lot of transformations that balloon each partition size, we will have problems.\n",
    "So when there are a lot of transformations that balloon each partition size making the initial \n",
    "partition size, say 50MB, would probably be better. \n",
    "\n",
    "6m30s, Matching the number of partitions to slots\n",
    "If there are 8 slots and 10 partitions, should we increase to 16 partitions or decrease to 8 partitions?\n",
    "... Answer: \n",
    "\n",
    "\n",
    "10m45s, Adaptive Queary Execution\n",
    "\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Demo notes\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lab notes\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
